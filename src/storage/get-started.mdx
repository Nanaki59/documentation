---
name: Get Started
menu: Storage
route: /storage/get-started
---

# Get Started

Once you've installed Kerberos Storage, you will have the Storage web application. This web application allows you to administrate Kerberos Storage. You can add storage providers, queues, accounts, and more.

## Login page

Once you open a browser, and navigate to the Storage web application (see installation for the url), you will land on the login page.

![Storage](../../public/images/factory/kerberos-factory-loginpage-small.png)

The default username password of the Storage applications is:

- username: **root**
- password: **kerberos**

> The username and password can be changed in the Kubernetes yaml file.

## License key

The first time you login you are asked for a license key, this can be obtained by sending a request to cedric@verstraeten.io. An example license key looks like this:

> -----BEGIN LICENSE KEY-----
L/+DAwEBB2xpY2Vuc2UB/4QAAQIBB1BheWxvYWQBCgABCVNpZ25hdHVyZQEKAAAA
/9z/hAFUPBAADG1haW4uTGljZW5zZf+BAwEBB0xpY2Vuc2UB/4IAAQIBB0NvbXBh
bnkBDAABB0V4cFRpbWUBBAAAABb/ghMBCmtlcmJlcm9zaW8B/MC7ZzIAAf+AOaJ+
8eU3OlZCphr4uJyH/PzfuDkMrqyfIsutc5kSz3rjsRtU4e6vETpHre8CPvFyx5w2
0O8PKCIR4z1q28qJK2cvHQTK0/zf+KfYLilEcmNczLXgg+bnPvtA/bU/K8ZEwbnw
Rta3+8zId+xtYQApdmHwo/Ih/vCN579iDeunwVQA
-----END LICENSE KEY-----

![Factory](../../public/images/factory/kerberos-factory-licensekey.png)

Once you received a license key, and entered it into the web application you will be redirected to the login screen. Enter your credentials again, and you will be guided into the dashboard.


## Providers

Once you signed in succesfully, you land on the providers page. Here is the place where you can add your datasources. You can choose from:

  - [Google Cloud Platform Storage](https://cloud.google.com/storage)
  - [Microsoft Azure Storage](https://azure.microsoft.com/en-us/services/storage/)
  - [Amazon Web Services S3](https://aws.amazon.com/s3/)
  - [Minio](https://min.io/)

For each provider, the appropriate security settings has to be filled in. For example for AWS you need to define the access and secret key, for GCP you need to define the Service Account.

Additionaly a provider can be marked as temporary. This is helpful if you wish to store your recordings for a short period, and want to use it for processing purpose only. A benefit of doing this is reducing costs (if you host in the public cloud).

![Storage](../../public/images/kerberos-storage-providers.png)

## Queues

Events or messages are generated each time a recording was uploaded to Kerberos Storage. These messages can be send to one ore moore message brokers such as Apache Kafka or AWS SQS. From the interface you can setup the connection, and once configured events will flow to your predefined message brokers.

An example or usecase what you do with Queue's is building something like our Kerberos Cloud offering. Each time a Kerberos Cloud customer is sending recordings to his/her cloud profile, it is stored through Kerberos Storage into one of our public data centers. Next to that a message is send into our private Kafka broker, which is starting a chain of processing:

  - Metadata storage in MongoDB,
  - Notifications,
  - Classification with Tensorflow,
  - Counting algorithms,
  - and much more.

![Storage](../../public/images/kerberos-storage-queues.png)

## Accounts

Accounts is the access for end users or customers. In the account section we define configurations such as:
  - Allowed providers,
  - Public Key,
  - Secret Key,
  - Day limit

An account can be generated for each individual, or a asteriks account (*) can be generated; which is more suitable if you want to seperate devices or regions of devices.
Once an account is created it can be used in Kerberos Enterprise (and soon Kerberos Open Source) to persist your data from your Kerberos Agent into your own datacenter, public or private cloud.

![Storage](../../public/images/kerberos-storage-accounts.png)

## Media

All recordings send through Kerberos Storage are stored, both for auditing as for cleanup policies.

![Storage](../../public/images/kerberos-storage-media.png)

## Cleanup

Storage might need to be erased after a while because of several reasons:

- maybe the data is no longer relevant
- due to security reasions
- etc.

To make this possible Kerberos Storage comes with a configurable cleanup script, which you can run inside your Kuberneters cluster next to your Kerberos Storage deployment.
